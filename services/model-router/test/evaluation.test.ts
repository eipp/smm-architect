import { describe, it, expect, beforeAll, afterAll, beforeEach } from '@jest/globals';
import { ModelEvaluationFramework, GoldenDatasetEntry } from '../src/services/ModelEvaluationFramework';
import { ModelRegistry } from '../src/services/ModelRegistry';
import { ModelMetadata } from '../src/types';

describe('Model Evaluation Framework Tests', () => {
  let evaluationFramework: ModelEvaluationFramework;
  let registry: ModelRegistry;
  let testModelId: string;

  beforeAll(async () => {
    registry = new ModelRegistry('redis://localhost:6379');
    evaluationFramework = new ModelEvaluationFramework(registry);
    
    // Register test model
    const testModel = await registry.registerModel({
      name: 'Test GPT-4',
      version: 'test-1.0',
      provider: 'openai',
      modelType: 'chat',
      capabilities: [{ type: 'text-generation', maxTokens: 4096 }],
      parameters: { temperature: 0.7, maxTokens: 4096 },
      status: 'active',
      tags: ['test']
    });
    testModelId = testModel.id;
  });

  afterAll(async () => {
    await registry.disconnect();
  });

  beforeEach(() => {
    // Reset state between tests
  });

  describe('Golden Dataset Management', () => {
    it('should load golden dataset successfully', async () => {
      const dataset: GoldenDatasetEntry[] = [
        {
          id: 'test-001',
          prompt: 'Write a creative marketing message for eco-friendly products',
          expectedOutput: 'Join the green revolution with our sustainable solutions...',
          metadata: {
            category: 'creativity',
            difficulty: 'medium',
            agentType: 'creative',
            tags: ['marketing', 'eco-friendly']
          },
          evaluationCriteria: {
            similarity: 0.7,
            semanticMatch: true,
            factualAccuracy: true,
            brandConsistency: true
          }
        },\n        {\n          id: 'test-002',\n          prompt: 'Generate a professional email response to a customer complaint',\n          expectedOutput: 'Thank you for bringing this to our attention...',\n          metadata: {\n            category: 'professional-communication',\n            difficulty: 'medium',\n            agentType: 'legal',\n            tags: ['customer-service', 'email']\n          },\n          evaluationCriteria: {\n            similarity: 0.8,\n            semanticMatch: true,\n            factualAccuracy: true,\n            brandConsistency: true\n          }\n        }\n      ];\n\n      await expect(\n        evaluationFramework.loadGoldenDataset('test-category', dataset)\n      ).resolves.not.toThrow();\n    });\n\n    it('should handle empty dataset', async () => {\n      await expect(\n        evaluationFramework.loadGoldenDataset('empty-category', [])\n      ).resolves.not.toThrow();\n    });\n  });\n\n  describe('Model Evaluation', () => {\n    beforeEach(async () => {\n      // Load test dataset\n      const dataset: GoldenDatasetEntry[] = [\n        {\n          id: 'eval-001',\n          prompt: 'Create a marketing slogan for a new smartphone',\n          expectedOutput: 'Innovation in your hands - the future is now',\n          metadata: {\n            category: 'marketing',\n            difficulty: 'easy',\n            agentType: 'creative',\n            tags: ['slogan', 'smartphone']\n          },\n          evaluationCriteria: {\n            similarity: 0.6,\n            semanticMatch: true,\n            factualAccuracy: false,\n            brandConsistency: true\n          }\n        },\n        {\n          id: 'eval-002',\n          prompt: 'Write a technical product description',\n          expectedOutput: 'Advanced features and cutting-edge technology...',\n          metadata: {\n            category: 'technical',\n            difficulty: 'hard',\n            agentType: 'research',\n            tags: ['technical', 'description']\n          },\n          evaluationCriteria: {\n            similarity: 0.8,\n            semanticMatch: true,\n            factualAccuracy: true,\n            brandConsistency: true\n          }\n        }\n      ];\n\n      await evaluationFramework.loadGoldenDataset('test-evaluation', dataset);\n    });\n\n    it('should evaluate model against golden dataset', async () => {\n      const evaluation = await evaluationFramework.evaluateModel(\n        testModelId,\n        'test-evaluation',\n        { sampleSize: 2, parallel: false }\n      );\n\n      expect(evaluation).toHaveProperty('modelId', testModelId);\n      expect(evaluation).toHaveProperty('category', 'test-evaluation');\n      expect(evaluation).toHaveProperty('overallScore');\n      expect(evaluation).toHaveProperty('passRate');\n      expect(evaluation).toHaveProperty('results');\n      expect(evaluation.results).toHaveLength(2);\n      expect(evaluation.summary.total).toBe(2);\n      expect(evaluation.overallScore).toBeGreaterThanOrEqual(0);\n      expect(evaluation.overallScore).toBeLessThanOrEqual(1);\n    });\n\n    it('should handle parallel evaluation', async () => {\n      const evaluation = await evaluationFramework.evaluateModel(\n        testModelId,\n        'test-evaluation',\n        { sampleSize: 2, parallel: true }\n      );\n\n      expect(evaluation.results).toHaveLength(2);\n      expect(evaluation.summary.total).toBe(2);\n    });\n\n    it('should sample dataset when sample size is smaller', async () => {\n      const evaluation = await evaluationFramework.evaluateModel(\n        testModelId,\n        'test-evaluation',\n        { sampleSize: 1 }\n      );\n\n      expect(evaluation.results).toHaveLength(1);\n      expect(evaluation.summary.total).toBe(1);\n    });\n\n    it('should handle non-existent model', async () => {\n      await expect(\n        evaluationFramework.evaluateModel(\n          'non-existent-model',\n          'test-evaluation'\n        )\n      ).rejects.toThrow('Model not found');\n    });\n\n    it('should handle non-existent dataset', async () => {\n      await expect(\n        evaluationFramework.evaluateModel(\n          testModelId,\n          'non-existent-dataset'\n        )\n      ).rejects.toThrow('Golden dataset not found');\n    });\n  });\n\n  describe('A/B Testing', () => {\n    let testModelBId: string;\n\n    beforeAll(async () => {\n      // Register second test model\n      const testModelB = await registry.registerModel({\n        name: 'Test Claude-3',\n        version: 'test-1.0',\n        provider: 'anthropic',\n        modelType: 'chat',\n        capabilities: [{ type: 'text-generation', maxTokens: 4096 }],\n        parameters: { temperature: 0.7, maxTokens: 4096 },\n        status: 'active',\n        tags: ['test']\n      });\n      testModelBId = testModelB.id;\n\n      // Load test dataset for A/B testing\n      const dataset: GoldenDatasetEntry[] = Array.from({ length: 10 }, (_, i) => ({\n        id: `ab-test-${i + 1}`,\n        prompt: `Test prompt ${i + 1} for A/B testing`,\n        expectedOutput: `Expected output ${i + 1}`,\n        metadata: {\n          category: 'ab-testing',\n          difficulty: 'medium',\n          agentType: 'creative',\n          tags: ['ab-test']\n        },\n        evaluationCriteria: {\n          similarity: 0.7,\n          semanticMatch: true,\n          factualAccuracy: true,\n          brandConsistency: true\n        }\n      }));\n\n      await evaluationFramework.loadGoldenDataset('ab-testing', dataset);\n    });\n\n    it('should run A/B test between two models', async () => {\n      const abTest = await evaluationFramework.runABTest(\n        testModelId,\n        testModelBId,\n        'ab-testing',\n        { sampleSize: 5, confidenceLevel: 0.95 }\n      );\n\n      expect(abTest).toHaveProperty('modelA', testModelId);\n      expect(abTest).toHaveProperty('modelB', testModelBId);\n      expect(abTest).toHaveProperty('testId');\n      expect(abTest.results.modelA).toHaveLength(5);\n      expect(abTest.results.modelB).toHaveLength(5);\n      expect(abTest.summary).toHaveProperty('winnerModel');\n      expect(abTest.summary).toHaveProperty('confidence');\n      expect(abTest.summary).toHaveProperty('statisticalSignificance');\n      expect(['modelA', 'modelB']).toContain(abTest.summary.winnerModel);\n    });\n\n    it('should handle different confidence levels', async () => {\n      const abTest = await evaluationFramework.runABTest(\n        testModelId,\n        testModelBId,\n        'ab-testing',\n        { sampleSize: 3, confidenceLevel: 0.90 }\n      );\n\n      expect(abTest.summary.confidence).toBeGreaterThanOrEqual(0);\n      expect(abTest.summary.confidence).toBeLessThanOrEqual(1);\n    });\n  });\n\n  describe('Drift Detection', () => {\n    beforeEach(async () => {\n      // Create some evaluation history for drift detection\n      const dataset: GoldenDatasetEntry[] = [\n        {\n          id: 'drift-001',\n          prompt: 'Test prompt for drift detection',\n          expectedOutput: 'Expected output',\n          metadata: {\n            category: 'drift-testing',\n            difficulty: 'medium',\n            agentType: 'creative',\n            tags: ['drift']\n          },\n          evaluationCriteria: {\n            similarity: 0.7,\n            semanticMatch: true,\n            factualAccuracy: true,\n            brandConsistency: true\n          }\n        }\n      ];\n\n      await evaluationFramework.loadGoldenDataset('drift-testing', dataset);\n      \n      // Generate some evaluation history\n      for (let i = 0; i < 3; i++) {\n        await evaluationFramework.evaluateModel(\n          testModelId,\n          'drift-testing',\n          { sampleSize: 1 }\n        );\n      }\n    });\n\n    it('should detect model drift', async () => {\n      const driftResult = await evaluationFramework.detectDrift(\n        testModelId,\n        24, // 24 hours\n        0.1 // 10% threshold\n      );\n\n      expect(driftResult).toHaveProperty('modelId', testModelId);\n      expect(driftResult).toHaveProperty('timeFrame', '24h');\n      expect(driftResult).toHaveProperty('driftDetected');\n      expect(driftResult).toHaveProperty('driftScore');\n      expect(driftResult).toHaveProperty('metrics');\n      expect(driftResult.metrics).toHaveProperty('qualityDrift');\n      expect(driftResult.metrics).toHaveProperty('performanceDrift');\n      expect(driftResult.metrics).toHaveProperty('outputDrift');\n      expect(driftResult.metrics).toHaveProperty('costDrift');\n      expect(driftResult).toHaveProperty('recommendations');\n      expect(Array.isArray(driftResult.recommendations)).toBe(true);\n    });\n\n    it('should handle different time frames', async () => {\n      const driftResult = await evaluationFramework.detectDrift(\n        testModelId,\n        1, // 1 hour\n        0.05 // 5% threshold\n      );\n\n      expect(driftResult.timeFrame).toBe('1h');\n      expect(driftResult.threshold).toBe(0.05);\n    });\n\n    it('should handle model with no history', async () => {\n      // Register a new model with no evaluation history\n      const newModel = await registry.registerModel({\n        name: 'New Test Model',\n        version: 'new-1.0',\n        provider: 'openai',\n        modelType: 'chat',\n        capabilities: [{ type: 'text-generation', maxTokens: 4096 }],\n        parameters: {},\n        status: 'active',\n        tags: ['new']\n      });\n\n      const driftResult = await evaluationFramework.detectDrift(newModel.id);\n      expect(driftResult.modelId).toBe(newModel.id);\n      // Should not crash, even with no history\n    });\n  });\n\n  describe('Continuous Monitoring', () => {\n    it('should start continuous monitoring', async () => {\n      await expect(\n        evaluationFramework.startContinuousMonitoring(1) // 1 minute for testing\n      ).resolves.not.toThrow();\n    });\n  });\n\n  describe('Model Benchmarking', () => {\n    it('should benchmark model performance', async () => {\n      const benchmark = await evaluationFramework.benchmarkModel(\n        testModelId,\n        'standard'\n      );\n\n      expect(benchmark).toHaveProperty('modelId', testModelId);\n      expect(benchmark).toHaveProperty('suite', 'standard');\n      expect(benchmark).toHaveProperty('scores');\n      expect(benchmark).toHaveProperty('ranking');\n      expect(benchmark).toHaveProperty('recommendations');\n      expect(typeof benchmark.scores).toBe('object');\n      expect(typeof benchmark.ranking).toBe('number');\n      expect(Array.isArray(benchmark.recommendations)).toBe(true);\n    });\n\n    it('should handle different benchmark suites', async () => {\n      const benchmark = await evaluationFramework.benchmarkModel(\n        testModelId,\n        'advanced'\n      );\n\n      expect(benchmark.suite).toBe('advanced');\n    });\n  });\n\n  describe('Evaluation Reports', () => {\n    beforeEach(async () => {\n      // Generate some evaluation history for reporting\n      const dataset: GoldenDatasetEntry[] = [\n        {\n          id: 'report-001',\n          prompt: 'Test prompt for reporting',\n          expectedOutput: 'Expected output',\n          metadata: {\n            category: 'reporting',\n            difficulty: 'medium',\n            agentType: 'creative',\n            tags: ['report']\n          },\n          evaluationCriteria: {\n            similarity: 0.7,\n            semanticMatch: true,\n            factualAccuracy: true,\n            brandConsistency: true\n          }\n        }\n      ];\n\n      await evaluationFramework.loadGoldenDataset('reporting', dataset);\n      \n      // Generate evaluation history\n      for (let i = 0; i < 5; i++) {\n        await evaluationFramework.evaluateModel(\n          testModelId,\n          'reporting',\n          { sampleSize: 1 }\n        );\n      }\n    });\n\n    it('should generate evaluation report', async () => {\n      const report = await evaluationFramework.generateEvaluationReport(\n        testModelId,\n        168 // 1 week\n      );\n\n      expect(report).toHaveProperty('modelId', testModelId);\n      expect(report).toHaveProperty('timeFrame', '168h');\n      expect(report).toHaveProperty('summary');\n      expect(report).toHaveProperty('trends');\n      expect(report).toHaveProperty('recommendations');\n      expect(Array.isArray(report.recommendations)).toBe(true);\n    });\n\n    it('should handle different time frames', async () => {\n      const report = await evaluationFramework.generateEvaluationReport(\n        testModelId,\n        24 // 24 hours\n      );\n\n      expect(report.timeFrame).toBe('24h');\n    });\n  });\n\n  describe('Event Handling', () => {\n    it('should emit events during evaluation', (done) => {\n      evaluationFramework.on('evaluationCompleted', (evaluation) => {\n        expect(evaluation).toHaveProperty('modelId');\n        expect(evaluation).toHaveProperty('overallScore');\n        done();\n      });\n\n      // Trigger evaluation\n      evaluationFramework.evaluateModel(\n        testModelId,\n        'creativity', // Default dataset\n        { sampleSize: 1 }\n      ).catch(done);\n    });\n\n    it('should emit events during A/B testing', (done) => {\n      // Register another model for A/B testing\n      registry.registerModel({\n        name: 'Test Model B',\n        version: 'test-b-1.0',\n        provider: 'anthropic',\n        modelType: 'chat',\n        capabilities: [{ type: 'text-generation', maxTokens: 4096 }],\n        parameters: {},\n        status: 'active',\n        tags: ['test-b']\n      }).then((modelB) => {\n        evaluationFramework.on('abTestCompleted', (comparison) => {\n          expect(comparison).toHaveProperty('testId');\n          expect(comparison).toHaveProperty('summary');\n          done();\n        });\n\n        // Trigger A/B test\n        evaluationFramework.runABTest(\n          testModelId,\n          modelB.id,\n          'creativity',\n          { sampleSize: 2 }\n        ).catch(done);\n      }).catch(done);\n    });\n\n    it('should emit events during drift detection', (done) => {\n      evaluationFramework.on('driftDetected', (driftResult) => {\n        expect(driftResult).toHaveProperty('modelId');\n        expect(driftResult).toHaveProperty('driftScore');\n        done();\n      });\n\n      // This might not trigger immediately as drift detection\n      // depends on having sufficient history and actual drift\n      setTimeout(() => {\n        done(); // Complete test if no drift detected\n      }, 1000);\n\n      // Try to detect drift\n      evaluationFramework.detectDrift(testModelId, 1, 0.001).catch(done);\n    });\n  });\n\n  describe('Error Handling', () => {\n    it('should handle invalid model IDs gracefully', async () => {\n      await expect(\n        evaluationFramework.evaluateModel(\n          'invalid-model-id',\n          'creativity'\n        )\n      ).rejects.toThrow();\n    });\n\n    it('should handle invalid dataset categories gracefully', async () => {\n      await expect(\n        evaluationFramework.evaluateModel(\n          testModelId,\n          'non-existent-category'\n        )\n      ).rejects.toThrow();\n    });\n\n    it('should handle empty evaluation results', async () => {\n      // Create a dataset that might cause evaluation failures\n      const problematicDataset: GoldenDatasetEntry[] = [\n        {\n          id: 'problem-001',\n          prompt: '', // Empty prompt\n          expectedOutput: '',\n          metadata: {\n            category: 'problematic',\n            difficulty: 'easy',\n            agentType: 'creative',\n            tags: []\n          },\n          evaluationCriteria: {\n            similarity: 1.0, // Impossible to achieve\n            semanticMatch: true,\n            factualAccuracy: true,\n            brandConsistency: true\n          }\n        }\n      ];\n\n      await evaluationFramework.loadGoldenDataset('problematic', problematicDataset);\n      \n      const evaluation = await evaluationFramework.evaluateModel(\n        testModelId,\n        'problematic',\n        { sampleSize: 1 }\n      );\n\n      // Should complete without crashing\n      expect(evaluation).toHaveProperty('results');\n      expect(evaluation.results).toHaveLength(1);\n    });\n  });\n\n  describe('Performance', () => {\n    it('should handle large datasets efficiently', async () => {\n      // Create a large dataset\n      const largeDataset: GoldenDatasetEntry[] = Array.from({ length: 100 }, (_, i) => ({\n        id: `large-${i + 1}`,\n        prompt: `Test prompt ${i + 1} for performance testing`,\n        expectedOutput: `Expected output ${i + 1}`,\n        metadata: {\n          category: 'performance',\n          difficulty: 'medium',\n          agentType: 'creative',\n          tags: ['performance']\n        },\n        evaluationCriteria: {\n          similarity: 0.7,\n          semanticMatch: true,\n          factualAccuracy: true,\n          brandConsistency: true\n        }\n      }));\n\n      await evaluationFramework.loadGoldenDataset('performance', largeDataset);\n      \n      const startTime = Date.now();\n      const evaluation = await evaluationFramework.evaluateModel(\n        testModelId,\n        'performance',\n        { sampleSize: 20, parallel: true }\n      );\n      const duration = Date.now() - startTime;\n\n      expect(evaluation.results).toHaveLength(20);\n      expect(duration).toBeLessThan(30000); // Should complete in under 30 seconds\n    });\n\n    it('should benefit from parallel processing', async () => {\n      const dataset: GoldenDatasetEntry[] = Array.from({ length: 10 }, (_, i) => ({\n        id: `parallel-${i + 1}`,\n        prompt: `Parallel test prompt ${i + 1}`,\n        expectedOutput: `Expected output ${i + 1}`,\n        metadata: {\n          category: 'parallel',\n          difficulty: 'medium',\n          agentType: 'creative',\n          tags: ['parallel']\n        },\n        evaluationCriteria: {\n          similarity: 0.7,\n          semanticMatch: true,\n          factualAccuracy: true,\n          brandConsistency: true\n        }\n      }));\n\n      await evaluationFramework.loadGoldenDataset('parallel', dataset);\n      \n      // Sequential evaluation\n      const startSequential = Date.now();\n      await evaluationFramework.evaluateModel(\n        testModelId,\n        'parallel',\n        { sampleSize: 10, parallel: false }\n      );\n      const sequentialDuration = Date.now() - startSequential;\n\n      // Parallel evaluation\n      const startParallel = Date.now();\n      await evaluationFramework.evaluateModel(\n        testModelId,\n        'parallel',\n        { sampleSize: 10, parallel: true }\n      );\n      const parallelDuration = Date.now() - startParallel;\n\n      // Parallel should be faster (though with mocked delays, might be close)\n      expect(parallelDuration).toBeLessThanOrEqual(sequentialDuration * 1.2); // Allow some variance\n    });\n  });\n});