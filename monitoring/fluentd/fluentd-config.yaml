apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
  labels:
    app: fluentd
    component: logging
data:
  fluent.conf: |
    # SMM Architect Centralized Logging Configuration
    # Input sources for log collection
    
    <source>
      @type tail
      @id smm_services_logs
      path /var/log/containers/smm-*.log
      pos_file /var/log/fluentd-smm-services.log.pos
      tag smm.services.*
      read_from_head true
      <parse>
        @type kubernetes
        expression /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
        time_format %Y-%m-%dT%H:%M:%S.%N%:z
      </parse>
    </source>

    <source>
      @type tail
      @id agent_logs
      path /var/log/containers/*-agent-*.log
      pos_file /var/log/fluentd-agents.log.pos
      tag smm.agents.*
      read_from_head true
      <parse>
        @type kubernetes
        expression /^(?<time>.+) (?<stream>stdout|stderr) [^ ]* (?<log>.*)$/
        time_format %Y-%m-%dT%H:%M:%S.%N%:z
      </parse>
    </source>

    <source>
      @type tail
      @id connector_logs
      path /var/log/connectors/*.log
      pos_file /var/log/fluentd-connectors.log.pos
      tag smm.connectors.*
      read_from_head true
      <parse>
        @type json
        time_key timestamp
        time_format %Y-%m-%dT%H:%M:%S.%L%z
      </parse>
    </source>

    <source>
      @type tail
      @id simulation_logs
      path /var/log/simulation/*.log
      pos_file /var/log/fluentd-simulation.log.pos
      tag smm.simulation.*
      read_from_head true
      <parse>
        @type json
        time_key timestamp
        time_format %Y-%m-%dT%H:%M:%S.%L%z
      </parse>
    </source>

    <source>
      @type tail
      @id n8n_logs
      path /var/log/n8n/*.log
      pos_file /var/log/fluentd-n8n.log.pos
      tag smm.workflows.*
      read_from_head true
      <parse>
        @type json
        time_key timestamp
        time_format %Y-%m-%dT%H:%M:%S.%L%z
      </parse>
    </source>

    # Filter for log enrichment and correlation
    <filter smm.**>
      @type record_transformer
      enable_ruby true
      <record>
        # Add service identification
        service_name ${tag_parts[1]}
        environment "#{ENV['ENVIRONMENT'] || 'production'}"
        cluster "#{ENV['CLUSTER_NAME'] || 'smm-prod'}"
        
        # Extract correlation IDs
        request_id ${record.dig("fields", "request_id") || record["request_id"] || record["requestId"] || "unknown"}
        workspace_id ${record.dig("fields", "workspace_id") || record["workspace_id"] || record["workspaceId"] || "unknown"}
        user_id ${record.dig("fields", "user_id") || record["user_id"] || record["userId"] || "unknown"}
        
        # Add timestamp standardization
        "@timestamp" ${time.strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
        
        # Extract log level
        level ${record["level"] || record["severity"] || "info"}
        
        # Add trace information
        trace_id ${record.dig("fields", "trace_id") || record["trace_id"] || record["traceId"] || ""}
        span_id ${record.dig("fields", "span_id") || record["span_id"] || record["spanId"] || ""}
      </record>
    </filter>

    # Parse structured logs from agents
    <filter smm.agents.**>
      @type parser
      key_name log
      reserve_data true
      inject_key_prefix agent_
      <parse>
        @type json
      </parse>
    </filter>

    # Parse and enrich connector logs
    <filter smm.connectors.**>
      @type record_transformer
      enable_ruby true
      <record>
        connector_type ${record.dig("fields", "connector") || record["connector"] || tag_parts[2]}
        platform ${record.dig("fields", "platform") || record["platform"] || "unknown"}
        api_endpoint ${record.dig("fields", "endpoint") || record["endpoint"] || ""}
        response_time ${record.dig("fields", "response_time") || record["response_time"] || 0}
        status_code ${record.dig("fields", "status_code") || record["status_code"] || 0}
      </record>
    </filter>

    # Parse simulation logs
    <filter smm.simulation.**>
      @type record_transformer
      enable_ruby true
      <record>
        simulation_id ${record.dig("fields", "simulation_id") || record["simulation_id"] || "unknown"}
        iteration ${record.dig("fields", "iteration") || record["iteration"] || 0}
        algorithm ${record.dig("fields", "algorithm") || record["algorithm"] || "monte_carlo"}
        readiness_score ${record.dig("fields", "readiness_score") || record["readiness_score"] || 0}
      </record>
    </filter>

    # Parse workflow logs
    <filter smm.workflows.**>
      @type record_transformer
      enable_ruby true
      <record>
        workflow_id ${record.dig("fields", "workflow_id") || record["workflow_id"] || "unknown"}
        execution_id ${record.dig("fields", "execution_id") || record["execution_id"] || "unknown"}
        node_type ${record.dig("fields", "node_type") || record["node_type"] || "unknown"}
        workflow_status ${record.dig("fields", "status") || record["status"] || "unknown"}
      </record>
    </filter>

    # Security log filtering
    <filter smm.**>
      @type grep
      <regexp>
        key log
        pattern /password|secret|token|key/i
      </regexp>
      <exclude>
        key message
        pattern /.*/
      </exclude>
    </filter>

    # Route critical errors to immediate alerting
    <match smm.** tag:error,fatal>
      @type copy
      <store>
        @type elasticsearch
        host elasticsearch.logging.svc.cluster.local
        port 9200
        index_name smm-critical-errors
        type_name _doc
        include_tag_key true
        tag_key @log_name
        <buffer>
          @type file
          path /var/log/fluentd-buffers/critical
          flush_mode interval
          flush_interval 5s
          chunk_limit_size 2MB
          total_limit_size 512MB
          overflow_action block
        </buffer>
      </store>
      <store>
        @type slack
        webhook_url "#{ENV['SLACK_WEBHOOK_CRITICAL']}"
        channel "#smm-critical-logs"
        username "LogAlert"
        color danger
        title "Critical Error in SMM Service"
        message "Service: %s\nError: %s\nTimestamp: %s"
        message_keys service_name,log,@timestamp
      </store>
    </match>

    # Security event routing
    <match smm.** tag:security>
      @type copy
      <store>
        @type elasticsearch
        host elasticsearch.logging.svc.cluster.local
        port 9200
        index_name smm-security-events
        type_name _doc
        include_tag_key true
        tag_key @log_name
        <buffer>
          @type file
          path /var/log/fluentd-buffers/security
          flush_mode interval
          flush_interval 10s
          chunk_limit_size 2MB
          total_limit_size 512MB
        </buffer>
      </store>
      <store>
        @type slack
        webhook_url "#{ENV['SLACK_WEBHOOK_SECURITY']}"
        channel "#security-alerts"
        username "SecurityAlert"
        color warning
        title "Security Event Detected"
      </store>
    </match>

    # Audit log routing
    <match smm.** tag:audit>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      index_name smm-audit-logs
      type_name _doc
      include_tag_key true
      tag_key @log_name
      <buffer>
        @type file
        path /var/log/fluentd-buffers/audit
        flush_mode interval
        flush_interval 30s
        chunk_limit_size 5MB
        total_limit_size 1GB
        retry_forever true
      </buffer>
    </match>

    # Performance metrics routing
    <match smm.** tag:performance>
      @type copy
      <store>
        @type elasticsearch
        host elasticsearch.logging.svc.cluster.local
        port 9200
        index_name smm-performance-logs
        type_name _doc
        include_tag_key true
        tag_key @log_name
        <buffer>
          @type file
          path /var/log/fluentd-buffers/performance
          flush_mode interval
          flush_interval 60s
          chunk_limit_size 10MB
          total_limit_size 2GB
        </buffer>
      </store>
      <store>
        @type prometheus
        <metric>
          name smm_log_performance_response_time
          type histogram
          desc SMM service response times from logs
          key response_time
          <labels>
            service ${service_name}
            endpoint ${api_endpoint}
          </labels>
        </metric>
      </store>
    </match>

    # General application logs
    <match smm.**>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      index_name smm-logs-#{Time.now.strftime('%Y.%m')}
      type_name _doc
      include_tag_key true
      tag_key @log_name
      time_key @timestamp
      include_timestamp true
      <buffer time>
        @type file
        path /var/log/fluentd-buffers/general
        timekey 1h
        timekey_wait 10m
        flush_mode interval
        flush_interval 30s
        chunk_limit_size 32MB
        total_limit_size 8GB
        overflow_action drop_oldest_chunk
      </buffer>
    </match>

    # Fallback for unmatched logs
    <match **>
      @type elasticsearch
      host elasticsearch.logging.svc.cluster.local
      port 9200
      index_name smm-fallback-logs
      type_name _doc
      include_tag_key true
      tag_key @log_name
      <buffer>
        @type file
        path /var/log/fluentd-buffers/fallback
        flush_mode interval
        flush_interval 60s
        chunk_limit_size 16MB
        total_limit_size 1GB
      </buffer>
    </match>

  kubernetes.conf: |
    # Kubernetes-specific log parsing
    <filter kubernetes.**>
      @type kubernetes_metadata
      @id filter_kube_metadata
      kubernetes_url "#{ENV['FLUENT_FILTER_KUBERNETES_URL'] || 'https://' + ENV.fetch('KUBERNETES_SERVICE_HOST') + ':' + ENV.fetch('KUBERNETES_SERVICE_PORT') + '/api'}"
      verify_ssl "#{ENV['KUBERNETES_VERIFY_SSL'] || true}"
      ca_file "#{ENV['KUBERNETES_CA_FILE']}"
      skip_labels "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_LABELS'] || 'false'}"
      skip_container_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_CONTAINER_METADATA'] || 'false'}"
      skip_master_url "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_MASTER_URL'] || 'false'}"
      skip_namespace_metadata "#{ENV['FLUENT_KUBERNETES_METADATA_SKIP_NAMESPACE_METADATA'] || 'false'}"
    </filter>

  prometheus.conf: |
    # Prometheus metrics from logs
    <filter smm.**>
      @type prometheus
      <metric>
        name smm_log_entries_total
        type counter
        desc Total number of log entries by service and level
        <labels>
          service ${service_name}
          level ${level}
          environment ${environment}
        </labels>
      </metric>
      <metric>
        name smm_error_log_entries_total
        type counter
        desc Total number of error log entries
        <labels>
          service ${service_name}
          error_type ${error_type}
        </labels>
      </metric>
    </filter>