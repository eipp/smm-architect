---
apiVersion: v1
kind: Namespace
metadata:
  name: logging
  labels:
    name: logging
    component: centralized-logging

---
# Elasticsearch Service
apiVersion: v1
kind: Service
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    component: logging
spec:
  selector:
    app: elasticsearch
  ports:
    - name: http
      port: 9200
      targetPort: 9200
    - name: transport
      port: 9300
      targetPort: 9300
  type: ClusterIP

---
# Elasticsearch Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: elasticsearch
  namespace: logging
  labels:
    app: elasticsearch
    component: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
        component: logging
    spec:
      securityContext:
        fsGroup: 1000
      initContainers:
        - name: fix-permissions
          image: busybox:1.35
          command: ['sh', '-c', 'chown -R 1000:1000 /usr/share/elasticsearch/data']
          securityContext:
            privileged: true
          volumeMounts:
            - name: elasticsearch-data
              mountPath: /usr/share/elasticsearch/data
        - name: increase-vm-max-map
          image: busybox:1.35
          command: ['sysctl', '-w', 'vm.max_map_count=262144']
          securityContext:
            privileged: true
      containers:
        - name: elasticsearch
          image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
          env:
            - name: discovery.type
              value: single-node
            - name: ES_JAVA_OPTS
              value: "-Xms1g -Xmx1g"
            - name: xpack.security.enabled
              value: "false"
            - name: xpack.monitoring.collection.enabled
              value: "true"
            - name: cluster.name
              value: smm-logging-cluster
            - name: node.name
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          ports:
            - containerPort: 9200
              name: http
            - containerPort: 9300
              name: transport
          resources:
            requests:
              memory: "2Gi"
              cpu: "500m"
            limits:
              memory: "4Gi"
              cpu: "1000m"
          volumeMounts:
            - name: elasticsearch-data
              mountPath: /usr/share/elasticsearch/data
            - name: elasticsearch-config
              mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
              subPath: elasticsearch.yml
              readOnly: true
          readinessProbe:
            httpGet:
              path: /_cluster/health
              port: 9200
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: /_cluster/health
              port: 9200
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
      volumes:
        - name: elasticsearch-data
          persistentVolumeClaim:
            claimName: elasticsearch-pvc
        - name: elasticsearch-config
          configMap:
            name: elasticsearch-config

---
# Elasticsearch Persistent Volume Claim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: elasticsearch-pvc
  namespace: logging
  labels:
    app: elasticsearch
    component: logging
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  storageClassName: fast-ssd

---
# Kibana Service
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
    component: logging
spec:
  selector:
    app: kibana
  ports:
    - name: http
      port: 5601
      targetPort: 5601
  type: LoadBalancer

---
# Kibana Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
  labels:
    app: kibana
    component: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
        component: logging
    spec:
      containers:
        - name: kibana
          image: docker.elastic.co/kibana/kibana:8.11.0
          env:
            - name: ELASTICSEARCH_HOSTS
              value: "http://elasticsearch:9200"
            - name: SERVER_NAME
              value: "kibana.smm-architect.com"
            - name: SERVER_HOST
              value: "0.0.0.0"
            - name: LOGGING_ROOT_LEVEL
              value: "info"
          ports:
            - containerPort: 5601
              name: http
          resources:
            requests:
              memory: "512Mi"
              cpu: "200m"
            limits:
              memory: "1Gi"
              cpu: "500m"
          readinessProbe:
            httpGet:
              path: /api/status
              port: 5601
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
          livenessProbe:
            httpGet:
              path: /api/status
              port: 5601
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10

---
# Fluentd ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
    component: logging

---
# Fluentd ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
  labels:
    app: fluentd
    component: logging
rules:
  - apiGroups: [""]
    resources: ["pods", "namespaces"]
    verbs: ["get", "list", "watch"]

---
# Fluentd ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
  labels:
    app: fluentd
    component: logging
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: fluentd
    namespace: logging

---
# Fluentd DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
  labels:
    app: fluentd
    component: logging
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
        component: logging
    spec:
      serviceAccountName: fluentd
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        - key: node-role.kubernetes.io/control-plane
          effect: NoSchedule
      containers:
        - name: fluentd
          image: fluent/fluentd-kubernetes-daemonset:v1.16-debian-elasticsearch7-1
          env:
            - name: FLUENT_ELASTICSEARCH_HOST
              value: "elasticsearch.logging.svc.cluster.local"
            - name: FLUENT_ELASTICSEARCH_PORT
              value: "9200"
            - name: FLUENT_ELASTICSEARCH_SCHEME
              value: "http"
            - name: FLUENTD_SYSTEMD_CONF
              value: "disable"
            - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH
              value: "/var/log/containers/fluent*"
            - name: FLUENT_CONTAINER_TAIL_PARSER_TYPE
              value: "cri"
            - name: FLUENT_ELASTICSEARCH_SSL_VERIFY
              value: "false"
            - name: ENVIRONMENT
              value: "production"
            - name: CLUSTER_NAME
              value: "smm-prod"
            - name: SLACK_WEBHOOK_CRITICAL
              valueFrom:
                secretKeyRef:
                  name: logging-secrets
                  key: slack-webhook-critical
            - name: SLACK_WEBHOOK_SECURITY
              valueFrom:
                secretKeyRef:
                  name: logging-secrets
                  key: slack-webhook-security
          resources:
            requests:
              memory: "200Mi"
              cpu: "100m"
            limits:
              memory: "500Mi"
              cpu: "200m"
          volumeMounts:
            - name: fluentd-config
              mountPath: /fluentd/etc/fluent.conf
              subPath: fluent.conf
            - name: fluentd-config
              mountPath: /fluentd/etc/kubernetes.conf
              subPath: kubernetes.conf
            - name: fluentd-config
              mountPath: /fluentd/etc/prometheus.conf
              subPath: prometheus.conf
            - name: varlog
              mountPath: /var/log
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
            - name: fluentd-buffer
              mountPath: /var/log/fluentd-buffers
          livenessProbe:
            httpGet:
              path: /fluentd.pod.healthcheck?json=%7B%22log%22%3A+%22health+check%22%7D
              port: 9880
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /fluentd.pod.healthcheck?json=%7B%22log%22%3A+%22health+check%22%7D
              port: 9880
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
      volumes:
        - name: fluentd-config
          configMap:
            name: fluentd-config
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: fluentd-buffer
          emptyDir: {}

---
# Logging Secrets
apiVersion: v1
kind: Secret
metadata:
  name: logging-secrets
  namespace: logging
  labels:
    app: logging
    component: logging
type: Opaque
data:
  # Base64 encoded Slack webhook URLs
  # Replace with actual base64 encoded webhook URLs
  slack-webhook-critical: aHR0cHM6Ly9ob29rcy5zbGFjay5jb20vc2VydmljZXMvWU9VUi9DUklUSUNBTC9XRUJIT09L
  slack-webhook-security: aHR0cHM6Ly9ob29rcy5zbGFjay5jb20vc2VydmljZXMvWU9VUi9TRUNVUklUWS9XRUJIT09L

---
# Log Retention CronJob
apiVersion: batch/v1
kind: CronJob
metadata:
  name: log-retention-cleanup
  namespace: logging
  labels:
    app: log-retention
    component: logging
spec:
  schedule: "0 2 * * *" # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          restartPolicy: OnFailure
          containers:
            - name: log-cleanup
              image: curlimages/curl:8.4.0
              command:
                - /bin/sh
                - -c
                - |
                  # Delete indices older than 30 days for general logs
                  curl -X DELETE "elasticsearch:9200/smm-logs-$(date -d '30 days ago' '+%Y.%m.*')" || true
                  
                  # Delete fallback logs older than 7 days
                  curl -X DELETE "elasticsearch:9200/smm-fallback-logs-$(date -d '7 days ago' '+%Y.%m.*')" || true
                  
                  # Keep audit logs for 7 years, security events for 7 years
                  # (These are handled by ILM policies)
                  
                  echo "Log cleanup completed"

---
# Network Policy for Logging Namespace
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: logging-network-policy
  namespace: logging
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    # Allow ingress from Fluentd to Elasticsearch
    - from:
        - podSelector:
            matchLabels:
              app: fluentd
      to:
        - podSelector:
            matchLabels:
              app: elasticsearch
      ports:
        - protocol: TCP
          port: 9200
    # Allow ingress from Kibana to Elasticsearch
    - from:
        - podSelector:
            matchLabels:
              app: kibana
      to:
        - podSelector:
            matchLabels:
              app: elasticsearch
      ports:
        - protocol: TCP
          port: 9200
    # Allow external access to Kibana
    - to:
        - podSelector:
            matchLabels:
              app: kibana
      ports:
        - protocol: TCP
          port: 5601
  egress:
    # Allow all egress for Fluentd (to collect logs from all namespaces)
    - from:
        - podSelector:
            matchLabels:
              app: fluentd
    # Allow Elasticsearch internal communication
    - from:
        - podSelector:
            matchLabels:
              app: elasticsearch
      to:
        - podSelector:
            matchLabels:
              app: elasticsearch
      ports:
        - protocol: TCP
          port: 9300
    # Allow DNS resolution
    - to: []
      ports:
        - protocol: UDP
          port: 53